{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画像認識による乗り物分類レシピ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://axross-recipe.com/recipes/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flickr API の準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の３つの乗り物の画像を分類する\n",
    "- 電車\n",
    "- 車\n",
    "- 自転車"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from flickrapi import FlickrAPI\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APIキー情報\n",
    "FLICKR_KEY = os.environ['FLICKR_KEY']\n",
    "FLICKR_SECRET = os.environ['FLICKR_SECRET']\n",
    "\n",
    "# 乗り物の名前\n",
    "VEHICLES = [\"train\", \"car\", \"bicycle\"]\n",
    "\n",
    "# 乗り物をループしてデータを取得\n",
    "for vehicle in VEHICLES:\n",
    "    # 保存フォルダの指定\n",
    "    save_dir = os.path.join(\"datasets\", vehicle)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Flickr APIの初期化\n",
    "    flickr = FlickrAPI(FLICKR_KEY, FLICKR_SECRET, format=\"parsed-json\")\n",
    "    \n",
    "    # 乗り物の名前を指定して100件の画像情報を取得\n",
    "    result = flickr.photos.search(text=vehicle,\n",
    "                                                      per_page=100,\n",
    "                                                      media=\"photos\",\n",
    "                                                      sort=\"relavance\",\n",
    "                                                      safe_search=1,\n",
    "                                                      extras=\"url_q, licence\")\n",
    "    \n",
    "    # 画像情報から実際の画像ファイルを取得\n",
    "    photos = result[\"photos\"]\n",
    "    for photo in photos[\"photo\"]:\n",
    "        # 画像のURL\n",
    "        url_q = photo[\"url_q\"]\n",
    "        # 画像のダウンロード先\n",
    "        filepath = os.path.join(save_dir, photo[\"id\"] + \".jpg\")\n",
    "        # 画像を指定したパスにダウンロードして保存\n",
    "        urlretrieve(url_q, filepath)\n",
    "        # クローリング先のサーバーに負荷を与えない\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットを交差検証用に分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乗り物の名前\n",
    "VEHICLES = [\"train\", \"car\", \"bicycle\"]\n",
    "# データセットのフォルダパス\n",
    "DATASET_DIR = os.path.join(\"datasets\")\n",
    "\n",
    "datasets = []\n",
    "labels = []\n",
    "\n",
    "for index, label in enumerate(VEHICLES):\n",
    "    # 画像の読み込み\n",
    "    photos_dir = os.path.join(DATASET_DIR, label)\n",
    "    filepaths = glob.glob(os.path.join(photos_dir, \"*.jpg\"))\n",
    "    \n",
    "    # 画像を順次処理してデータセットを作成\n",
    "    for i, filepath in enumerate(filepaths):\n",
    "        # 各乗り物のデータを100に揃える\n",
    "        if i >= 100:\n",
    "            break\n",
    "        \n",
    "        # 画像の読み込み\n",
    "        image = Image.open(filepath)\n",
    "        # RGBの3色に変換\n",
    "        image = image.convert(\"RGB\")\n",
    "        # 画像のサイズを統一\n",
    "        image = image.resize((50, 50))\n",
    "        # 画像を数値の配列に変換\n",
    "        dataset = np.asarray(image)\n",
    "        # データセットを追加\n",
    "        datasets.append(dataset)\n",
    "        # ラベルを追加\n",
    "        labels.append(index)\n",
    "\n",
    "# Tensorflow がデータを処理しやすいように numpy の array に変換\n",
    "datasets = np.array(datasets)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# データセットとラベルの両方を学習データとテストデータに分類\n",
    "dataset_train, dataset_test, label_train, label_test = model_selection.train_test_split(datasets, labels)\n",
    "\n",
    "# 分類したデータをファイルに保存\n",
    "data = (dataset_train, dataset_test, label_train, label_test)\n",
    "np.save(os.path.join(DATASET_DIR, \"vehicle.npy\"), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルのトレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import (Activation, Conv2D, Dense, Dropout, Flatten, MaxPooling2D)\n",
    "from keras.models import Sequentialfrom keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乗り物の名前\n",
    "VEHICLES = [\"train\", \"car\", \"bicycle\"]\n",
    "# データセットのフォリダパス\n",
    "DATASET_DIR = os.path.join(\"datasets\")\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_train, dataset_test, label_train, label_test = np.load(os.path.join(DATASET_DIR, \"vehicle.npy\"), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
